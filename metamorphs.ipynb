{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":103669,"databundleVersionId":12900657,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n# +\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Lung Cancer Detection**","metadata":{}},{"cell_type":"code","source":"import pandas as pd","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Loading the Dataset","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/idealize-2025-datathon-competition/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/idealize-2025-datathon-competition/test.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T14:44:17.904765Z","iopub.execute_input":"2025-07-14T14:44:17.905493Z","iopub.status.idle":"2025-07-14T14:44:21.725346Z","shell.execute_reply.started":"2025-07-14T14:44:17.905468Z","shell.execute_reply":"2025-07-14T14:44:21.724765Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Data exploration","metadata":{}},{"cell_type":"code","source":"df_train.isna().sum()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test.isna().sum()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train.describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test.describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Survived to Not Survived Ratio\nsurvived = df_train['survival_status'].value_counts()[1]\nnot_survived = df_train['survival_status'].value_counts()[0]\n\nsurvived_ratio = survived/df_train.shape[0]\nnot_survived_ratio =  not_survived/df_train.shape[0]\n\nprint(f\"Survived ratio : {survived_ratio} \\nNot survived ratio : {not_survived_ratio}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train['survival_status'].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Pre-processing\n### Data preprocessing Steps\n- Filling `cigarattes_per_day` \n- Columns to encode(Binary) : `family_cancer_history`, `has_other_cancer`, `asthma_diagnosis`, `liver_condition`, `blood_pressure_status`\n- Columns to encode (OneHot) : `residence_state`, `smoking_status`, `treatment_type`","metadata":{"execution":{"iopub.status.busy":"2025-07-04T13:43:06.471176Z","iopub.execute_input":"2025-07-04T13:43:06.471498Z","iopub.status.idle":"2025-07-04T13:43:06.484624Z","shell.execute_reply.started":"2025-07-04T13:43:06.471453Z","shell.execute_reply":"2025-07-04T13:43:06.483557Z"}}},{"cell_type":"code","source":"# Importing the necessory Libraries \nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nimport pandas as pd\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass ProcessSmoking(BaseEstimator,TransformerMixin):\n    def fit(self,X,y=None):\n        return self\n        # Current Smokers - no NaNs\n        # Never Smoked - no NaNs\n        # Former Smokers - no NaNs\n        # Passive smokers - contain NaNs\n    def transform(self,X):\n        X.loc[X['smoking_status'].isin(['Never Smoked', 'Passive Smoker', 'Passive', 'Non Smoker']), 'cigarettes_per_day'] = 0\n        return X\n    \n\nclass ProcessCols(BaseEstimator,TransformerMixin):\n    def fit(self,X,y=None):\n        return self\n\n    def transform(self,X):\n        # Encode 'family_cancer_history': Yes â†’ 1, No â†’ 0\n        X['family_cancer_history'] = X['family_cancer_history'].replace({'Yes': 1, 'No': 0})\n        \n        # Encode 'has_other_cancer': Yes â†’ 1, No â†’ 0\n        X['has_other_cancer'] = X['has_other_cancer'].replace({'Yes': 1, 'No': 0})\n        \n        # Encode 'asthma_diagnosis': Yes â†’ 1, No â†’ 0\n        X['asthma_diagnosis'] = X['asthma_diagnosis'].replace({'Yes': 1, 'No': 0})\n        \n        # Clean and encode 'blood_pressure_status'\n        X['blood_pressure_status'] = X['blood_pressure_status'].replace({\n            'High Blood Pressure': 1,\n            'Elevated': 1,\n            'Normal': 0,\n            'Normal BP': 0  # Treating 'Normal' and 'Normal BP' as same\n        })\n        \n        # Clean and encode 'liver_condition'\n        X['liver_condition'] = X['liver_condition'].replace({\n            'Normal': 0,\n            'Normal Liver': 0,\n            'Liver OK': 0,\n            'No Issue': 0,\n            'Has Cirrhosis': 1,\n            'Cirrhos': 1  # Assuming typo or shorthand for cirrhosis\n        })\n        X['sex'] = X['sex'].replace({'Male': 1, 'Female': 0})\n        X['smoking_status'] = X['smoking_status'].replace({\n            'Current Smoker': 'Current',\n            'Former Smoker': 'Former',\n            'Passive Smoker': 'Passive',\n            'Never Smoked': 'Never',\n            'Passive': 'Passive',\n            'Non Smoker': 'Never',\n            'Former Smk': 'Former'\n        })\n        X['treatment_type'] = X['treatment_type'].replace({\n            'Chemo': 'Chemotherapy',\n            'Surg': 'Surgery',\n            'Combo': 'Combined'\n        })\n        X = X.drop('residence_state', axis=1)\n        \n        return X\n\nclass EncodeCatCols(BaseEstimator, TransformerMixin):\n    def __init__(self, columns=['smoking_status', 'treatment_type']):\n        self.columns = columns\n        self.encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n\n    def fit(self, X, y=None):\n        self.encoder.fit(X[self.columns])\n        return self\n\n    def transform(self, X):\n        encoded = self.encoder.transform(X[self.columns])\n        \n        # Get column names from OneHotEncoder\n        column_names = self.encoder.get_feature_names_out(self.columns)\n        \n        # Create DataFrame from encoded matrix\n        encoded_df = pd.DataFrame(encoded, columns=column_names, index=X.index)\n        # Drop original categorical columns and join encoded ones\n        X = X.drop(columns=self.columns)\n        X = pd.concat([X, encoded_df], axis=1)\n        \n        return X\n\n\nclass FormatDates(BaseEstimator,TransformerMixin):\n    def fit(self,X,y=None):\n        return self\n\n    def transform(self,X):\n        X['treatment_start_date'] = pd.to_datetime(X['treatment_start_date'], errors='coerce')\n        X['treatment_end_date'] = pd.to_datetime(X['treatment_end_date'], errors='coerce')\n        X['diagnosis_date'] = pd.to_datetime(X['diagnosis_date'], errors='coerce')\n        X['treatment_duration'] = (X['treatment_end_date'] - X['treatment_start_date']).dt.days\n        X['diagnosis_to_treatment_delay'] = (X['treatment_start_date'] - X['diagnosis_date']).dt.days\n        \n        X = X.drop(['treatment_end_date', 'treatment_start_date', 'diagnosis_date'], axis=1)\n        return X\n\nclass DropUnwantedCols(BaseEstimator,TransformerMixin):\n    def fit(self,X,y=None):\n        return self\n\n    def transform(self,X):\n        X = X.drop(['first_name', 'last_name', 'record_id'], axis=1)\n        return X\n\n\n\n\n# Building Data Pre-Processing pipeline\ndata_pipeline = Pipeline([\n    ('process_smoking', ProcessSmoking()),\n    ('process_columns', ProcessCols()),\n    ('format_dates', FormatDates()),\n    ('encode_categoricals', EncodeCatCols(columns=['smoking_status', 'treatment_type'])),\n    ('drop_unwanted', DropUnwantedCols()),\n    ('scaler', StandardScaler())  \n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T14:58:08.024252Z","iopub.execute_input":"2025-07-14T14:58:08.024821Z","iopub.status.idle":"2025-07-14T14:58:08.038896Z","shell.execute_reply.started":"2025-07-14T14:58:08.024795Z","shell.execute_reply":"2025-07-14T14:58:08.038159Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"## Separating Independent Variables (X) and Dependent Variable (y)\n","metadata":{}},{"cell_type":"code","source":"X = df_train.drop('survival_status', axis=1)\ny = df_train['survival_status']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Pre-processing data ","metadata":{}},{"cell_type":"code","source":"X_transformed = data_pipeline.fit_transform(X)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_test_transformed = data_pipeline.transform(df_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Approach - I","metadata":{}},{"cell_type":"markdown","source":"## Model Building","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models, callbacks\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report, roc_auc_score\nfrom sklearn.utils import class_weight\nimport numpy as np\n\n# 1. Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.2, random_state=42, stratify=y)\n\n# 2. Scale features\n# -- done --\n\n# 3. Compute class weights\nweights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\nclass_weights = dict(enumerate(weights))\nprint(\"Class Weights:\", class_weights)\n\n# 4. Define the neural network with dropout\nmodel = models.Sequential([\n    layers.Input(shape=(X_train.shape[1],)),\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.3),\n    layers.Dense(64, activation='relu'),\n    layers.Dropout(0.3),\n    layers.Dense(32, activation='relu'),\n    layers.Dense(1, activation='sigmoid')  # Binary output\n])\n\n# 5. Compile with additional metrics\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=[\n                  'accuracy',\n                  tf.keras.metrics.Precision(name='precision'),\n                  tf.keras.metrics.Recall(name='recall'),\n                  tf.keras.metrics.AUC(name='auc')\n              ])\n\n# 6. Early stopping\nearly_stop = callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n\n# 7. Train the model\nhistory = model.fit(X_train, y_train,\n                    validation_split=0.1,\n                    epochs=5,\n                    batch_size=64,\n                    class_weight=class_weights,\n                    callbacks=[early_stop],\n                    verbose=1)\n\n# 8. Evaluate\nloss, acc, prec, rec, auc = model.evaluate(X_test, y_test)\nprint(f\"\\nâœ… Test Accuracy: {acc:.4f} | Precision: {prec:.4f} | Recall: {rec:.4f} | AUC: {auc:.4f}\")\n\n# 9. Predictions\ny_pred = (model.predict(X_test) > 0.49).astype(\"int32\")\n\n# 10. Full classification report\nprint(\"\\nğŸ“Š Classification Report:\\n\")\nprint(classification_report(y_test, y_pred, digits=4))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test_transformed_scaled = data_pipeline.transform(df_test_transformed)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test_transformed_scaled\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred_prob = (model.predict(df_test_transformed_scaled)).flatten()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred_prob","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n* 0.498501 - 33.1%\n* 0.4984 - 33.9%\n* 0.4982 - 35%\n* 0.498 - 35.3%\n* 0.493 - 35.7%\n* 0.49 - 35.8%\n\n","metadata":{}},{"cell_type":"code","source":"y_pred = (y_pred_prob > 0.49).astype(\"int32\") \n\nimport numpy as np\nunique, counts = np.unique(y_pred, return_counts=True)\nprint(dict(zip(unique, counts)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission = pd.DataFrame({\n    'record_id' : df_test['record_id'], \n    'survival_status' : y_pred.flatten()\n})\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission['survival_status'].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Exporting ","metadata":{}},{"cell_type":"code","source":"import joblib\nmodel.save('cancer_prediction_model.h5')\njoblib.dump(data_pipeline, 'data_pipeline.pkl')\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfig, axs = plt.subplots(1, 3, figsize=(18, 5))\n\n# Accuracy\naxs[0].plot(history.history['accuracy'], label='Train')\naxs[0].plot(history.history['val_accuracy'], label='Val')\naxs[0].set_title('Accuracy')\naxs[0].set_xlabel('Epochs')\naxs[0].set_ylabel('Accuracy')\naxs[0].legend()\naxs[0].grid(True)\n\n# Precision\naxs[1].plot(history.history['precision'], label='Train')\naxs[1].plot(history.history['val_precision'], label='Val')\naxs[1].set_title('Precision')\naxs[1].set_xlabel('Epochs')\naxs[1].set_ylabel('Precision')\naxs[1].legend()\naxs[1].grid(True)\n\n# Recall\naxs[2].plot(history.history['recall'], label='Train')\naxs[2].plot(history.history['val_recall'], label='Val')\naxs[2].set_title('Recall')\naxs[2].set_xlabel('Epochs')\naxs[2].set_ylabel('Recall')\naxs[2].legend()\naxs[2].grid(True)\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Approach - II","metadata":{}},{"cell_type":"markdown","source":"## Handling Imbalanced - Under Sampling","metadata":{}},{"cell_type":"code","source":"df_train_survived = df_train[df_train['survival_status'] == 1]\ndf_train_survived['survival_status'].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train_not_survived = df_train[df_train['survival_status'] == 0]\ndf_train_not_survived['survival_status'].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train_not_survived_sampled = df_train_not_survived.sample(219604)\ndf_train_not_survived_sampled.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train_sampled = pd.concat([df_train_not_survived_sampled, df_train_survived], axis=0)\ndf_train_sampled.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = df_train_sampled.drop('survival_status', axis=1)\ny = df_train_sampled['survival_status']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_sampled_transformed = data_pipeline.fit_transform(X)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models, callbacks\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report, roc_auc_score\nfrom sklearn.utils import class_weight\nimport numpy as np\n\n# 1. Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X_train_sampled_transformed, y, test_size=0.2, random_state=42) # , stratify=y\n\n# 2. Scale features\n# -- done --\n\n# 3. Compute class weights\n# weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n# class_weights = dict(enumerate(weights))\n# print(\"Class Weights:\", class_weights)\n\n# 4. Define the neural network with dropout\nmodel = models.Sequential([\n    layers.Input(shape=(X_train.shape[1],)),\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.3),\n    layers.Dense(64, activation='relu'),\n    layers.Dropout(0.3),\n    layers.Dense(32, activation='relu'),\n    layers.Dense(1, activation='sigmoid')  # Binary output\n])\n\n# 5. Compile with additional metrics\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=[\n                  'accuracy',\n                  tf.keras.metrics.Precision(name='precision'),\n                  tf.keras.metrics.Recall(name='recall'),\n                  tf.keras.metrics.AUC(name='auc')\n              ])\n\n# 6. Early stopping\nearly_stop = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n\n# 7. Train the model\nhistory = model.fit(X_train, y_train,\n                    validation_split=0.1,\n                    epochs=20,\n                    batch_size=3000,\n                    # class_weight=class_weights,\n                    callbacks=[early_stop],\n                    verbose=1)\n\n# 8. Evaluate\nloss, acc, prec, rec, auc = model.evaluate(X_test, y_test)\nprint(f\"\\nâœ… Test Accuracy: {acc:.4f} | Precision: {prec:.4f} | Recall: {rec:.4f} | AUC: {auc:.4f}\")\n\n# 9. Predictions\ny_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n\n# 10. Full classification report\nprint(\"\\nğŸ“Š Classification Report:\\n\")\nprint(classification_report(y_test, y_pred, digits=4))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test_transformed = data_pipeline.transform(df_test)\ndf_test_transformed.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save('MetaMorphs_model.h5')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred_prob = (model.predict(df_test_transformed)).flatten()\ny_pred = (y_pred_prob > 0.5).astype(\"int32\") \n\nimport numpy as np\nunique, counts = np.unique(y_pred, return_counts=True)\nprint(dict(zip(unique, counts)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission = pd.DataFrame({\n    'record_id' : df_test['record_id'], \n    'survival_status' : y_pred.flatten()\n})\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"min(y_pred_prob), max(y_pred_prob)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.remove(\"/kaggle/working/submission.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Approach - III","metadata":{}},{"cell_type":"markdown","source":"## Pipeline with Advanced Feature Engineering","metadata":{}},{"cell_type":"code","source":"# Improved preprocessing with better feature engineering\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nimport pandas as pd\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport numpy as np\n\nclass ProcessSmoking(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n        \n    def transform(self, X):\n        X = X.copy()\n        # Set cigarettes_per_day to 0 for non-smokers\n        X.loc[X['smoking_status'].isin(['Never Smoked', 'Passive Smoker', 'Passive', 'Non Smoker']), 'cigarettes_per_day'] = 0\n        \n        # Handle missing values in cigarettes_per_day\n        X['cigarettes_per_day'] = X['cigarettes_per_day'].fillna(0)\n        \n        return X\n\nclass ProcessCols(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        X = X.copy()\n        \n        # Binary encodings\n        X['family_cancer_history'] = X['family_cancer_history'].replace({'Yes': 1, 'No': 0})\n        X['has_other_cancer'] = X['has_other_cancer'].replace({'Yes': 1, 'No': 0})\n        X['asthma_diagnosis'] = X['asthma_diagnosis'].replace({'Yes': 1, 'No': 0})\n        X['sex'] = X['sex'].replace({'Male': 1, 'Female': 0})\n        \n        # Blood pressure with better handling\n        X['blood_pressure_status'] = X['blood_pressure_status'].replace({\n            'High Blood Pressure': 2,\n            'Elevated': 1,\n            'Normal': 0,\n            'Normal BP': 0\n        })\n        \n        # Liver condition\n        X['liver_condition'] = X['liver_condition'].replace({\n            'Normal': 0, 'Normal Liver': 0, 'Liver OK': 0, 'No Issue': 0,\n            'Has Cirrhosis': 1, 'Cirrhos': 1\n        })\n        \n        # Smoking status standardization\n        X['smoking_status'] = X['smoking_status'].replace({\n            'Current Smoker': 'Current',\n            'Former Smoker': 'Former',\n            'Passive Smoker': 'Passive',\n            'Never Smoked': 'Never',\n            'Passive': 'Passive',\n            'Non Smoker': 'Never',\n            'Former Smk': 'Former'\n        })\n        \n        # Treatment type\n        X['treatment_type'] = X['treatment_type'].replace({\n            'Chemo': 'Chemotherapy',\n            'Surg': 'Surgery',\n            'Combo': 'Combined'\n        })\n        \n        return X\n\nclass FeatureEngineering(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n        \n    def transform(self, X):\n        X = X.copy()\n        \n        # BMI calculation\n        X['bmi'] = X['weight_kg'] / (X['height_cm'] / 100) ** 2\n        \n        # Age groups\n        X['age_group'] = pd.cut(X['patient_age'], \n                               bins=[0, 40, 55, 70, 100], \n                               labels=['young', 'middle', 'senior', 'elderly'])\n        \n        # Smoking intensity categories\n        X['smoking_intensity'] = pd.cut(X['cigarettes_per_day'], \n                                       bins=[-1, 0, 10, 20, 100], \n                                       labels=['none', 'light', 'moderate', 'heavy'])\n        \n        # Cancer stage severity\n        X['stage_severity'] = X['cancer_stage'].replace({1: 'early', 2: 'early', 3: 'advanced', 4: 'advanced'})\n        \n        # Risk factors combination\n        X['high_risk_combo'] = (\n            (X['cancer_stage'] >= 3) & \n            (X['cigarettes_per_day'] > 10) & \n            (X['family_cancer_history'] == 1)\n        ).astype(int)\n        \n        return X\n\nclass FormatDates(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        X = X.copy()\n        \n        # Convert dates\n        X['treatment_start_date'] = pd.to_datetime(X['treatment_start_date'], errors='coerce')\n        X['treatment_end_date'] = pd.to_datetime(X['treatment_end_date'], errors='coerce')\n        X['diagnosis_date'] = pd.to_datetime(X['diagnosis_date'], errors='coerce')\n        \n        # Calculate durations\n        X['treatment_duration'] = (X['treatment_end_date'] - X['treatment_start_date']).dt.days\n        X['diagnosis_to_treatment_delay'] = (X['treatment_start_date'] - X['diagnosis_date']).dt.days\n        \n        # Handle negative or extreme values\n        X['treatment_duration'] = X['treatment_duration'].clip(0, 365*2)  # Max 2 years\n        X['diagnosis_to_treatment_delay'] = X['diagnosis_to_treatment_delay'].clip(0, 365)  # Max 1 year\n        \n        # Fill missing values\n        X['treatment_duration'] = X['treatment_duration'].fillna(X['treatment_duration'].median())\n        X['diagnosis_to_treatment_delay'] = X['diagnosis_to_treatment_delay'].fillna(X['diagnosis_to_treatment_delay'].median())\n        \n        # Extract year and month for potential seasonality\n        X['diagnosis_year'] = X['diagnosis_date'].dt.year\n        X['diagnosis_month'] = X['diagnosis_date'].dt.month\n        \n        # Drop original date columns\n        X = X.drop(['treatment_end_date', 'treatment_start_date', 'diagnosis_date'], axis=1)\n        \n        return X\n\nclass EncodeCatCols(BaseEstimator, TransformerMixin):\n    def __init__(self, columns=['smoking_status', 'treatment_type', 'age_group', 'smoking_intensity', 'stage_severity']):\n        self.columns = columns\n        self.encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n\n    def fit(self, X, y=None):\n        self.encoder.fit(X[self.columns])\n        return self\n\n    def transform(self, X):\n        X = X.copy()\n        encoded = self.encoder.transform(X[self.columns])\n        column_names = self.encoder.get_feature_names_out(self.columns)\n        encoded_df = pd.DataFrame(encoded, columns=column_names, index=X.index)\n        X = X.drop(columns=self.columns)\n        X = pd.concat([X, encoded_df], axis=1)\n        return X\n\nclass DropUnwantedCols(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        drop_cols = ['first_name', 'last_name', 'record_id', 'residence_state']\n        existing_cols = [col for col in drop_cols if col in X.columns]\n        X = X.drop(existing_cols, axis=1)\n        return X\n\n# Improved pipeline\ndata_pipeline = Pipeline([\n    ('process_smoking', ProcessSmoking()),\n    ('process_columns', ProcessCols()),\n    ('feature_engineering', FeatureEngineering()),\n    ('format_dates', FormatDates()),\n    ('encode_categoricals', EncodeCatCols()),\n    ('drop_unwanted', DropUnwantedCols()),\n    ('scaler', StandardScaler())  \n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T14:44:25.800185Z","iopub.execute_input":"2025-07-14T14:44:25.800493Z","iopub.status.idle":"2025-07-14T14:44:25.818870Z","shell.execute_reply.started":"2025-07-14T14:44:25.800471Z","shell.execute_reply":"2025-07-14T14:44:25.818000Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"df_train_survived = df_train[df_train['survival_status'] == 1]\ndf_train_survived['survival_status'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T14:58:18.407801Z","iopub.execute_input":"2025-07-14T14:58:18.408625Z","iopub.status.idle":"2025-07-14T14:58:18.470745Z","shell.execute_reply.started":"2025-07-14T14:58:18.408597Z","shell.execute_reply":"2025-07-14T14:58:18.469966Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"survival_status\n1    219604\nName: count, dtype: int64"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"df_train_not_survived = df_train[df_train['survival_status'] == 0]\ndf_train_not_survived['survival_status'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T14:58:08.121801Z","iopub.execute_input":"2025-07-14T14:58:08.122020Z","iopub.status.idle":"2025-07-14T14:58:08.295262Z","shell.execute_reply.started":"2025-07-14T14:58:08.121996Z","shell.execute_reply":"2025-07-14T14:58:08.294535Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"survival_status\n0    780395\nName: count, dtype: int64"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"### Under Sampling","metadata":{}},{"cell_type":"code","source":"df_train_not_survived_sampled = df_train_not_survived.sample(400000)\ndf_train_not_survived_sampled.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T15:00:06.247755Z","iopub.execute_input":"2025-07-14T15:00:06.248272Z","iopub.status.idle":"2025-07-14T15:00:06.525202Z","shell.execute_reply.started":"2025-07-14T15:00:06.248250Z","shell.execute_reply":"2025-07-14T15:00:06.524575Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"(400000, 22)"},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"### Over Sampling","metadata":{}},{"cell_type":"code","source":"df_train_survived_sampled = df_train_survived.sample(400000,replace=True)\ndf_train_survived_sampled.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T15:00:15.988064Z","iopub.execute_input":"2025-07-14T15:00:15.988804Z","iopub.status.idle":"2025-07-14T15:00:16.118995Z","shell.execute_reply.started":"2025-07-14T15:00:15.988778Z","shell.execute_reply":"2025-07-14T15:00:16.118330Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"(400000, 22)"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"df_train_sampled = pd.concat([df_train_not_survived_sampled, df_train_survived_sampled], axis=0)\ndf_train_sampled.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T15:00:35.080619Z","iopub.execute_input":"2025-07-14T15:00:35.080918Z","iopub.status.idle":"2025-07-14T15:00:35.262109Z","shell.execute_reply.started":"2025-07-14T15:00:35.080899Z","shell.execute_reply":"2025-07-14T15:00:35.261528Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"(800000, 22)"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"X = df_train_sampled.drop('survival_status', axis=1)\ny = df_train_sampled['survival_status']\n\nX_transformed = data_pipeline.fit_transform(X)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T15:00:50.078269Z","iopub.execute_input":"2025-07-14T15:00:50.078569Z","iopub.status.idle":"2025-07-14T15:00:53.760745Z","shell.execute_reply.started":"2025-07-14T15:00:50.078549Z","shell.execute_reply":"2025-07-14T15:00:53.760099Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30517/863333924.py:25: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  X['family_cancer_history'] = X['family_cancer_history'].replace({'Yes': 1, 'No': 0})\n/tmp/ipykernel_30517/863333924.py:28: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  X['has_other_cancer'] = X['has_other_cancer'].replace({'Yes': 1, 'No': 0})\n/tmp/ipykernel_30517/863333924.py:31: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  X['asthma_diagnosis'] = X['asthma_diagnosis'].replace({'Yes': 1, 'No': 0})\n/tmp/ipykernel_30517/863333924.py:34: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  X['blood_pressure_status'] = X['blood_pressure_status'].replace({\n/tmp/ipykernel_30517/863333924.py:42: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  X['liver_condition'] = X['liver_condition'].replace({\n/tmp/ipykernel_30517/863333924.py:50: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  X['sex'] = X['sex'].replace({'Male': 1, 'Female': 0})\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models, callbacks\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report, roc_auc_score\nfrom sklearn.utils import class_weight\nimport numpy as np\n\n# 1. Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.2, random_state=42) # , stratify=y\n\n# 2. Scale features\n# -- done --\n\n# 3. Compute class weights\n# weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n# class_weights = dict(enumerate(weights))\n# print(\"Class Weights:\", class_weights)\n\n# 4. Define the neural network with dropout\nmodel = models.Sequential([\n    layers.Input(shape=(X_train.shape[1],)),\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.3),\n    layers.Dense(64, activation='relu'),\n    layers.Dropout(0.3),\n    layers.Dense(32, activation='relu'),\n    layers.Dense(1, activation='sigmoid')  # Binary output\n])\n\n# 5. Compile with additional metrics\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=[\n                  'accuracy',\n                  tf.keras.metrics.Precision(name='precision'),\n                  tf.keras.metrics.Recall(name='recall'),\n                  tf.keras.metrics.AUC(name='auc')\n              ])\n\n# 6. Early stopping\nearly_stop = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n\n# 7. Train the model\nhistory = model.fit(X_train, y_train,\n                    validation_split=0.1,\n                    epochs=20,\n                    batch_size=3000,\n                    # class_weight=class_weights,\n                    callbacks=[early_stop],\n                    verbose=1)\n\n# 8. Evaluate\nloss, acc, prec, rec, auc = model.evaluate(X_test, y_test)\nprint(f\"\\nâœ… Test Accuracy: {acc:.4f} | Precision: {prec:.4f} | Recall: {rec:.4f} | AUC: {auc:.4f}\")\n\n# 9. Predictions\ny_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n\n# 10. Full classification report\nprint(\"\\nğŸ“Š Classification Report:\\n\")\nprint(classification_report(y_test, y_pred, digits=4))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T15:01:06.069599Z","iopub.execute_input":"2025-07-14T15:01:06.069874Z","iopub.status.idle":"2025-07-14T15:01:39.814069Z","shell.execute_reply.started":"2025-07-14T15:01:06.069856Z","shell.execute_reply":"2025-07-14T15:01:39.813235Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/20\n\u001b[1m192/192\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 13ms/step - accuracy: 0.4994 - auc: 0.4983 - loss: 0.6978 - precision: 0.4988 - recall: 0.4983 - val_accuracy: 0.4983 - val_auc: 0.4990 - val_loss: 0.6932 - val_precision: 0.4980 - val_recall: 0.4471\nEpoch 2/20\n\u001b[1m192/192\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5006 - auc: 0.4999 - loss: 0.6935 - precision: 0.4997 - recall: 0.4817 - val_accuracy: 0.5008 - val_auc: 0.5017 - val_loss: 0.6932 - val_precision: 0.5006 - val_recall: 0.5294\nEpoch 3/20\n\u001b[1m192/192\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5014 - auc: 0.5021 - loss: 0.6933 - precision: 0.5009 - recall: 0.5043 - val_accuracy: 0.5050 - val_auc: 0.5064 - val_loss: 0.6931 - val_precision: 0.5088 - val_recall: 0.2793\nEpoch 4/20\n\u001b[1m192/192\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5013 - auc: 0.5017 - loss: 0.6933 - precision: 0.4999 - recall: 0.4152 - val_accuracy: 0.5041 - val_auc: 0.5068 - val_loss: 0.6931 - val_precision: 0.5031 - val_recall: 0.6291\nEpoch 5/20\n\u001b[1m192/192\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5038 - auc: 0.5033 - loss: 0.6931 - precision: 0.5034 - recall: 0.4525 - val_accuracy: 0.5040 - val_auc: 0.5042 - val_loss: 0.6931 - val_precision: 0.5031 - val_recall: 0.6232\nEpoch 6/20\n\u001b[1m192/192\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5015 - auc: 0.5034 - loss: 0.6931 - precision: 0.5009 - recall: 0.5301 - val_accuracy: 0.5058 - val_auc: 0.5044 - val_loss: 0.6931 - val_precision: 0.5049 - val_recall: 0.5785\nEpoch 7/20\n\u001b[1m192/192\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5027 - auc: 0.5035 - loss: 0.6931 - precision: 0.5020 - recall: 0.4754 - val_accuracy: 0.5030 - val_auc: 0.5051 - val_loss: 0.6931 - val_precision: 0.5017 - val_recall: 0.8493\nEpoch 8/20\n\u001b[1m192/192\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5029 - auc: 0.5049 - loss: 0.6931 - precision: 0.5023 - recall: 0.5742 - val_accuracy: 0.5047 - val_auc: 0.5071 - val_loss: 0.6931 - val_precision: 0.5030 - val_recall: 0.7746\n\u001b[1m5000/5000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - accuracy: 0.5035 - auc: 0.5016 - loss: 0.6932 - precision: 0.5101 - recall: 0.2784\n\nâœ… Test Accuracy: 0.5029 | Precision: 0.5050 | Recall: 0.2780 | AUC: 0.5019\n\u001b[1m5000/5000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step\n\nğŸ“Š Classification Report:\n\n              precision    recall  f1-score   support\n\n           0     0.5021    0.7276    0.5941     80018\n           1     0.5050    0.2780    0.3586     79982\n\n    accuracy                         0.5029    160000\n   macro avg     0.5035    0.5028    0.4764    160000\nweighted avg     0.5035    0.5029    0.4764    160000\n\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}